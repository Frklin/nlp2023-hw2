{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"./hw2/stud/\")\n",
    "from torch.utils.data import DataLoader\n",
    "import config\n",
    "# from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from load import *\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from pytorch_lightning import Trainer\n",
    "from glossBERT import GlossBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# FINE_TRAIN_DATA\n",
    "# FINE_VAL_DATA\n",
    "# FINE_TEST_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = CoarseGrainedDataset(config.COARSE_TRAIN_DATA)\n",
    "# val_dataset = CoarseGrainedDataset(config.COARSE_VAL_DATA)\n",
    "# test_dataset = CoarseGrainedDataset(config.COARSE_TEST_DATA)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# mapping = load_map(config.MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = FineGrainedDataset(config.FINE_TRAIN_DATA)\n",
    "val_dataset = FineGrainedDataset(config.FINE_VAL_DATA)\n",
    "test_dataset = FineGrainedDataset(config.FINE_TEST_DATA)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=glossBERT_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=glossBERT_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=glossBERT_collate_fn)\n",
    "\n",
    "mapping = load_map(config.MAP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Bailly', ',', 'after', 'leaving', 'Fort Snelling', 'in', 'August', '1821', ',', 'was', 'forced', 'to', 'leave', 'some', 'of', 'the', 'cattle', 'at', 'the', \"Hudson's Bay Company's\", 'post', 'on', 'Lake Traverse', '``', 'in', 'the', 'Sieux Country', \"''\", 'and', 'reached', 'Fort Garry', ',', 'as', 'the', 'Selkirk', \"Hudson's Bay Company\", 'center', 'was', 'now', 'called', ',', 'late', 'in', 'the', 'fall', '.', 'He', 'set out', 'on', 'his', '700', '-', '\"mile\"', 'return', 'journey', 'with', 'five', 'families', 'of', 'discontented', 'and', 'disappointed', 'Swiss', 'who', 'turned', 'their', 'eyes', 'toward', 'the', 'United States', '.', '[SEP]', 'mile:', 'a', 'unit', 'of', 'length', 'used', 'in', 'navigation;', 'exactly', '1,852', 'meters;', 'historically', 'based', 'on', 'the', 'distance', 'spanned', 'by', 'one', 'minute', 'of', 'arc', 'in', 'latitude', '[SEP]'], ['person', ',', 'after', 'leave', 'location', 'in', 'august', '1821', ',', 'be', 'force', 'to', 'leave', 'some', 'of', 'the', 'cattle', 'at', 'the', 'group', 'post', 'on', 'location', '``', 'in', 'the', 'location', \"''\", 'and', 'reach', 'location', ',', 'as', 'the', 'person', 'group', 'center', 'be', 'now', 'call', ',', 'late', 'in', 'the', 'fall', '.', 'he', 'set_out', 'on', 'he', '700', '-', 'mile', 'return', 'journey', 'with', 'five', 'family', 'of', 'discontented', 'and', 'disappoint', 'swiss', 'who', 'turn', 'they', 'eye', 'toward', 'the', 'united_states', '.'], ['NOUN', '.', 'ADP', 'VERB', 'NOUN', 'ADP', 'NOUN', 'NUM', '.', 'VERB', 'VERB', 'PRT', 'VERB', 'DET', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'ADP', 'NOUN', '.', 'ADP', 'DET', 'NOUN', '.', 'CONJ', 'VERB', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'VERB', '.', 'ADJ', 'ADP', 'DET', 'NOUN', '.', 'PRON', 'VERB', 'ADP', 'PRON', 'NUM', '.', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'CONJ', 'VERB', 'NOUN', 'PRON', 'VERB', 'PRON', 'NOUN', 'ADP', 'DET', 'NOUN', '.'], 'nautical_mile.n.02', 0, 53)\n"
     ]
    }
   ],
   "source": [
    "# take a look at the data \n",
    "#load one data fr om the train_dataset\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # display one item\n",
    "# for batch in train_loader:\n",
    "#     input_ids, senses_ids, labels, attention_mask = batch\n",
    "#     # pritn one item\n",
    "#     print(input_ids)\n",
    "#     print(senses_ids)\n",
    "#     print(labels)\n",
    "#     print(attention_mask)\n",
    "\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = GlossBERT()\n",
    "# model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1566: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | bert       | RobertaModel     | 124 M \n",
      "1 | relu       | ReLU             | 0     \n",
      "2 | classifier | Linear           | 1.5 K \n",
      "3 | loss       | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.589   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/francesco/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/5909 [00:00<?, ?it/s] tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0])\n",
      "Epoch 0:   0%|          | 1/5909 [03:37<357:19:07, 217.73s/it, loss=0.638, v_num=1, train_loss=0.638, train_acc=0.750]tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch 0:   0%|          | 2/5909 [03:52<190:53:56, 116.34s/it, loss=0.589, v_num=1, train_loss=0.539, train_acc=0.938]tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1])\n",
      "Epoch 0:   0%|          | 3/5909 [04:01<132:07:30, 80.54s/it, loss=0.604, v_num=1, train_loss=0.634, train_acc=0.688] tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "Epoch 0:   0%|          | 4/5909 [04:14<104:13:56, 63.55s/it, loss=0.574, v_num=1, train_loss=0.485, train_acc=0.875]tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:682: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  return trainer_fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# TRAINER\n",
    "# wandb_logger = WandbLogger(project='nlp_hw2', log_model=True)\n",
    "trainer = Trainer(gpus=0, max_epochs=10, callbacks=[ModelCheckpoint(monitor='val_loss')])#, logger=wandb_logger)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "# trainer = Trainer(max_epochs=10, callbacks=[ModelCheckpoint(monitor='val_loss')])\n",
    "# trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
