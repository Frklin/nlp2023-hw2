{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"./hw2/stud/\")\n",
    "from torch.utils.data import DataLoader\n",
    "import config\n",
    "# from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from load import *\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from pytorch_lightning import Trainer\n",
    "from glossBERT import GlossBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# FINE_TRAIN_DATA\n",
    "# FINE_VAL_DATA\n",
    "# FINE_TEST_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = FineGrainedDataset(config.FINE_TRAIN_DATA)\n",
    "val_dataset = FineGrainedDataset(config.FINE_VAL_DATA)\n",
    "test_dataset = FineGrainedDataset(config.FINE_TEST_DATA)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=glossBERT_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=glossBERT_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=glossBERT_collate_fn)\n",
    "\n",
    "mapping = load_map(config.MAP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<s>', 'Bailly', ',', 'after', 'leaving', 'Fort Snelling', 'in', 'August', '1821', ',', 'was', 'forced', 'to', 'leave', 'some', 'of', 'the', 'cattle', 'at', 'the', \"Hudson's Bay Company's\", 'post', 'on', 'Lake Traverse', '``', 'in', 'the', 'Sieux Country', \"''\", 'and', 'reached', 'Fort Garry', ',', 'as', 'the', 'Selkirk', \"Hudson's Bay Company\", 'center', 'was', 'now', 'called', ',', 'late', 'in', 'the', 'fall', '.', 'He', 'set out', 'on', 'his', '700', '-', 'mile', 'return', 'journey', 'with', 'five', 'families', 'of', 'discontented', 'and', 'disappointed', 'Swiss', 'who', 'turned', 'their', 'eyes', 'toward', 'the', 'United States', '.', '</s>', 'mile:', 'a', 'unit', 'of', 'length', 'used', 'in', 'navigation;', 'exactly', '1,852', 'meters;', 'historically', 'based', 'on', 'the', 'distance', 'spanned', 'by', 'one', 'minute', 'of', 'arc', 'in', 'latitude', '</s>'], ['person', ',', 'after', 'leave', 'location', 'in', 'august', '1821', ',', 'be', 'force', 'to', 'leave', 'some', 'of', 'the', 'cattle', 'at', 'the', 'group', 'post', 'on', 'location', '``', 'in', 'the', 'location', \"''\", 'and', 'reach', 'location', ',', 'as', 'the', 'person', 'group', 'center', 'be', 'now', 'call', ',', 'late', 'in', 'the', 'fall', '.', 'he', 'set_out', 'on', 'he', '700', '-', 'mile', 'return', 'journey', 'with', 'five', 'family', 'of', 'discontented', 'and', 'disappoint', 'swiss', 'who', 'turn', 'they', 'eye', 'toward', 'the', 'united_states', '.'], ['NOUN', '.', 'ADP', 'VERB', 'NOUN', 'ADP', 'NOUN', 'NUM', '.', 'VERB', 'VERB', 'PRT', 'VERB', 'DET', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'ADP', 'NOUN', '.', 'ADP', 'DET', 'NOUN', '.', 'CONJ', 'VERB', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'VERB', '.', 'ADJ', 'ADP', 'DET', 'NOUN', '.', 'PRON', 'VERB', 'ADP', 'PRON', 'NUM', '.', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'CONJ', 'VERB', 'NOUN', 'PRON', 'VERB', 'PRON', 'NOUN', 'ADP', 'DET', 'NOUN', '.'], 'nautical_mile.n.02', 0, 53, 'd049.s041.t001', 73)\n"
     ]
    }
   ],
   "source": [
    "# take a look at the data \n",
    "#load one data fr om the train_dataset\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # display one item\n",
    "# for batch in train_loader:\n",
    "#     input_ids, senses_ids, labels, attention_mask = batch\n",
    "#     # pritn one item\n",
    "#     print(input_ids)\n",
    "#     print(senses_ids)\n",
    "#     print(labels)\n",
    "#     print(attention_mask)\n",
    "\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = GlossBERT()\n",
    "# model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/francesco/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1566: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name       | Type                          | Params\n",
      "-------------------------------------------------------------\n",
      "0 | roberta    | RobertaForTokenClassification | 124 M \n",
      "1 | bert       | RobertaModel                  | 124 M \n",
      "2 | relu       | ReLU                          | 0     \n",
      "3 | classifier | Linear                        | 1.5 K \n",
      "4 | loss       | CrossEntropyLoss              | 0     \n",
      "-------------------------------------------------------------\n",
      "248 M     Trainable params\n",
      "0         Non-trainable params\n",
      "248 M     Total params\n",
      "994.815   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<', (0, 1)), ('s', (1, 2)), ('>', (2, 3)), ('ĠA', (3, 5)), ('Ġsecond', (5, 12)), ('Ġtwitched', (12, 21)), ('Ġhis', (21, 25)), ('Ġshirtsleeve', (25, 37)), ('Ġ,', (37, 39)), ('Ġand', (39, 43)), ('Ġhe', (43, 46)), ('Ġfelt', (46, 51)), ('Ġa', (51, 53)), ('Ġbrief', (53, 59)), ('Ġburn', (59, 64)), ('Ġon', (64, 67)), ('Ġhis', (67, 71)), ('Ġupper', (71, 77)), ('Ġarm', (77, 81)), ('Ġ.', (81, 83)), ('ĠAnother', (83, 91)), ('Ġsnarled', (91, 99)), ('Ġclose', (99, 105)), ('Ġoverhead', (105, 114)), ('Ġ.', (114, 116)), ('Ġ</', (116, 119)), ('s', (119, 120)), ('>', (120, 121)), ('Ġsnarled', (121, 129)), (':', (129, 130)), ('Ġtwist', (130, 136)), ('Ġtogether', (136, 145)), ('Ġor', (145, 148)), ('Ġentwine', (148, 156)), ('Ġinto', (156, 161)), ('Ġa', (161, 163)), ('Ġconfusing', (163, 173)), ('Ġmass', (173, 178)), ('Ġ</', (178, 181)), ('s', (181, 182)), ('>', (182, 183))]\n",
      "[('<', (0, 1)), ('s', (1, 2)), ('>', (2, 3)), ('ĠFirst', (3, 9)), ('Ġof', (9, 12)), ('Ġall', (12, 16)), ('Ġthere', (16, 22)), ('Ġwas', (22, 26)), ('Ġthe', (26, 30)), ('Ġparsonage', (30, 40)), ('Ġ,', (40, 42)), ('Ġan', (42, 45)), ('Ġutterly', (45, 53)), ('Ġimpossible', (53, 64)), ('Ġplace', (64, 70)), ('Ġfor', (70, 74)), ('Ġcivilized', (74, 84)), ('Ġpeople', (84, 91)), ('Ġto', (91, 94)), ('Ġlive', (94, 99)), ('Ġin', (99, 102)), ('Ġ,', (102, 104)), ('Ġoriginally', (104, 115)), ('Ġpoorly', (115, 122)), ('Ġconceived', (122, 132)), ('Ġ,', (132, 134)), ('Ġapparently', (134, 145)), ('Ġnot', (145, 149)), ('Ġrepaired', (149, 158)), ('Ġfor', (158, 162)), ('Ġyears', (162, 168)), ('Ġ,', (168, 170)), ('Ġwith', (170, 175)), ('Ġno', (175, 178)), ('Ġplumbing', (178, 187)), ('Ġor', (187, 190)), ('Ġsewage', (190, 197)), ('Ġ,', (197, 199)), ('Ġwith', (199, 204)), ('Ġrat', (204, 208)), ('-', (208, 209)), ('holes', (209, 214)), ('Ġand', (214, 218)), ('Ġrot', (218, 222)), ('Ġ.', (222, 224)), ('Ġ</', (224, 227)), ('s', (227, 228)), ('>', (228, 229)), ('Ġrepaired', (229, 238)), (':', (238, 239)), ('Ġset', (239, 243)), ('Ġstraight', (243, 252)), ('Ġor', (252, 255)), ('Ġright', (255, 261)), ('Ġ</', (261, 264)), ('s', (264, 265)), ('>', (265, 266))]\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "['<s>', 'First of all', 'there', 'was', 'the', 'parsonage', ',', 'an', 'utterly', 'impossible', 'place', 'for', 'civilized', 'people', 'to', 'live in', ',', 'originally', 'poorly', 'conceived', ',', 'apparently', 'not', 'repaired', 'for', 'years', ',', 'with', 'no', 'plumbing', 'or', 'sewage', ',', 'with', 'rat-holes', 'and', 'rot', '.', '</s>', 'repaired:', 'set', 'straight', 'or', 'right', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "/home/francesco/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:682: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  return trainer_fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# TRAINER\n",
    "# wandb_logger = WandbLogger(project='nlp_hw2', log_model=True)\n",
    "trainer = Trainer(gpus=0, max_epochs=10, callbacks=[ModelCheckpoint(monitor='val_loss')])#, logger=wandb_logger)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "# trainer = Trainer(max_epochs=10, callbacks=[ModelCheckpoint(monitor='val_loss')])\n",
    "# trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing BertForTokenClassification: ['roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.output.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'lm_head.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'classifier.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'classifier.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The bank can guarantee deposits will eventually cover future tuition costs because it invests in adjustable-rate mortgage securities.\"\n",
    "sense = \"bank%1:14:00::\"\n",
    "definition = \"a financial institution that accepts deposits and channels the money into lending activities\"\n",
    "max_seq_length = 128\n",
    "target_idx = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\",  do_lower_case=True)\n",
    "\n",
    "tokens_a = tokenizer.tokenize(sentence)\n",
    "tokens_b = tokenizer.tokenize(definition)\n",
    "\n",
    "tokens = [config.CLS_TOKEN] + tokens_a + [\"[SEP]\"]\n",
    "segment_ids = [0] * len(tokens)\n",
    "tokens += tokens_b + [\"[SEP]\"]\n",
    "segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "# tokens are attended to.\n",
    "input_mask = [1] * len(input_ids)\n",
    "\n",
    "padding = [0] * (max_seq_length - len(input_ids))\n",
    "input_ids += padding\n",
    "input_mask += padding\n",
    "segment_ids += padding\n",
    "\n",
    "from transformers import BertForTokenClassification\n",
    "bert = BertForTokenClassification.from_pretrained(\"roberta-base\", num_labels=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
