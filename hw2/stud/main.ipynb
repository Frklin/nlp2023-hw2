{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/francesco/Desktop/University/MNLP/nlp2023-hw2/hw2/stud\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"./hw2/stud/\")\n",
    "from torch.utils.data import DataLoader\n",
    "import config\n",
    "# from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from load import *\n",
    "from model import ConSecModel\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "# from trainer import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# FINE_TRAIN_DATA\n",
    "# FINE_VAL_DATA\n",
    "# FINE_TEST_DATA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/francesco/Desktop/University/MNLP/nlp2023-hw2/hw2/stud\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = FineGrainedDataset(config.FINE_TRAIN_DATA)\n",
    "val_dataset = FineGrainedDataset(config.FINE_VAL_DATA)\n",
    "test_dataset = FineGrainedDataset(config.FINE_TEST_DATA)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "mapping = load_map(config.MAP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/intermediate/sense_embeddings.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m sense_embeddings \u001b[39m=\u001b[39m compute_definition_embeddigns()\n\u001b[1;32m      4\u001b[0m \u001b[39m# save sense embeddings in a file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m np\u001b[39m.\u001b[39;49msave(config\u001b[39m.\u001b[39;49mSENSE_EMBEDDINGS_PATH, sense_embeddings)\n\u001b[1;32m      6\u001b[0m \u001b[39m# # model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# model = ConSecModel(num_senses=sense_embeddings)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# # Loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# criterion = torch.nn.CrossEntropyLoss()\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/numpy/lib/npyio.py:518\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    517\u001b[0m         file \u001b[39m=\u001b[39m file \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 518\u001b[0m     file_ctx \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(file, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    520\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/intermediate/sense_embeddings.npy'"
     ]
    }
   ],
   "source": [
    "# sense embeddigs\n",
    "sense_embeddings = compute_definition_embeddigns()\n",
    "\n",
    "# save sense embeddings in a file\n",
    "np.save(config.SENSE_EMBEDDINGS_PATH, sense_embeddings)\n",
    "# # model\n",
    "# model = ConSecModel(num_senses=sense_embeddings)\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Loss\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name             | Type             | Params\n",
      "------------------------------------------------------\n",
      "0 | bert             | RobertaModel     | 124 M \n",
      "1 | sense_embeddings | Embedding        | 3.4 M \n",
      "2 | fc               | Linear           | 3.4 M \n",
      "3 | loss_fn          | CrossEntropyLoss | 0     \n",
      "------------------------------------------------------\n",
      "131 M     Trainable params\n",
      "0         Non-trainable params\n",
      "131 M     Total params\n",
      "526.101   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_23220/4116552099.py\", line 3, in <module>\n",
      "    trainer.fit(model, train_loader, val_loader)\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 531, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 570, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 975, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 1016, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py\", line 287, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py\", line 379, in validation_step\n",
      "    return self.model.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/Desktop/University/MNLP/nlp2023-hw2/hw2/stud/model.py\", line 88, in validation_step\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/Desktop/University/MNLP/nlp2023-hw2/hw2/stud/model.py\", line 67, in forward\n",
      "    candidate_representations = self.sense_embeddings(candidate_indices)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n",
      "    return F.embedding(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/functional.py\", line 2210, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: index out of range in self\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/home/francesco/miniconda3/envs/NLP/lib/python3.11/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=10, callbacks=[ModelCheckpoint(monitor='val_loss')])\n",
    "# trainer.train_epoch()\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"I went to the bank to deposit my money\"\n",
    "# words = sentence.split()\n",
    "# candidates = {4: [\"river\", \"river2\", \"money\"], 8: [\"money\", \"richness\"]}\n",
    "# definitions = {\"river\": \"a large natural stream of water (larger than a creek)\", \"river2\" : \"a financial institution that accepts deposits and channels the money into lending activities\", \"money\": \"the most common medium of exchange\", \"richness\": \"the property of being extremely abundant\"}\n",
    "# senses = {4: 2, 8: 0}\n",
    "\n",
    "# new_input_sequences = []\n",
    "# for idx in candidates.keys():\n",
    "#     input_sequence = ['<s>'] + words[:idx] + ['<d>', words[idx], '</d>'] + words[idx+1:]\n",
    "#     for candidate in candidates[idx]:\n",
    "#         input_sequence.extend(['<def>'] + definitions[candidate].split())\n",
    "#     for other_idx in candidates.keys():\n",
    "#         if other_idx != idx:\n",
    "#             input_sequence.extend(['<GT>'] + definitions[candidates[other_idx][senses[other_idx]]].split())\n",
    "#     input_sequence.append('</s>')\n",
    "#     new_input_sequences.append(input_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
