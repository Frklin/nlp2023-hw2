{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"./hw2/stud/\")\n",
    "from torch.utils.data import DataLoader\n",
    "import config\n",
    "# from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from load import *\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from pytorch_lightning import Trainer\n",
    "from glossBERT import GlossBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# FINE_TRAIN_DATA\n",
    "# FINE_VAL_DATA\n",
    "# FINE_TEST_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = CoarseGrainedDataset(config.COARSE_TRAIN_DATA)\n",
    "# val_dataset = CoarseGrainedDataset(config.COARSE_VAL_DATA)\n",
    "# test_dataset = CoarseGrainedDataset(config.COARSE_TEST_DATA)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# mapping = load_map(config.MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = FineGrainedDataset(config.FINE_TRAIN_DATA)\n",
    "val_dataset = FineGrainedDataset(config.FINE_VAL_DATA)\n",
    "test_dataset = FineGrainedDataset(config.FINE_TEST_DATA)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=glossBERT_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=glossBERT_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=glossBERT_collate_fn)\n",
    "\n",
    "mapping = load_map(config.MAP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<s>', 'Bailly', ',', 'after', 'leaving', 'Fort Snelling', 'in', 'August', '1821', ',', 'was', 'forced', 'to', 'leave', 'some', 'of', 'the', 'cattle', 'at', 'the', \"Hudson's Bay Company's\", 'post', 'on', 'Lake Traverse', '``', 'in', 'the', 'Sieux Country', \"''\", 'and', 'reached', 'Fort Garry', ',', 'as', 'the', 'Selkirk', \"Hudson's Bay Company\", 'center', 'was', 'now', 'called', ',', 'late', 'in', 'the', 'fall', '.', 'He', 'set out', 'on', 'his', '700', '-', '\"mile\"', 'return', 'journey', 'with', 'five', 'families', 'of', 'discontented', 'and', 'disappointed', 'Swiss', 'who', 'turned', 'their', 'eyes', 'toward', 'the', 'United States', '.', '</s>', 'mile:', 'a', 'unit', 'of', 'length', 'used', 'in', 'navigation;', 'exactly', '1,852', 'meters;', 'historically', 'based', 'on', 'the', 'distance', 'spanned', 'by', 'one', 'minute', 'of', 'arc', 'in', 'latitude', '</s>'], ['person', ',', 'after', 'leave', 'location', 'in', 'august', '1821', ',', 'be', 'force', 'to', 'leave', 'some', 'of', 'the', 'cattle', 'at', 'the', 'group', 'post', 'on', 'location', '``', 'in', 'the', 'location', \"''\", 'and', 'reach', 'location', ',', 'as', 'the', 'person', 'group', 'center', 'be', 'now', 'call', ',', 'late', 'in', 'the', 'fall', '.', 'he', 'set_out', 'on', 'he', '700', '-', 'mile', 'return', 'journey', 'with', 'five', 'family', 'of', 'discontented', 'and', 'disappoint', 'swiss', 'who', 'turn', 'they', 'eye', 'toward', 'the', 'united_states', '.'], ['NOUN', '.', 'ADP', 'VERB', 'NOUN', 'ADP', 'NOUN', 'NUM', '.', 'VERB', 'VERB', 'PRT', 'VERB', 'DET', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'ADP', 'NOUN', '.', 'ADP', 'DET', 'NOUN', '.', 'CONJ', 'VERB', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'VERB', '.', 'ADJ', 'ADP', 'DET', 'NOUN', '.', 'PRON', 'VERB', 'ADP', 'PRON', 'NUM', '.', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'CONJ', 'VERB', 'NOUN', 'PRON', 'VERB', 'PRON', 'NOUN', 'ADP', 'DET', 'NOUN', '.'], 'nautical_mile.n.02', 0, 53)\n"
     ]
    }
   ],
   "source": [
    "# take a look at the data \n",
    "#load one data fr om the train_dataset\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # display one item\n",
    "# for batch in train_loader:\n",
    "#     input_ids, senses_ids, labels, attention_mask = batch\n",
    "#     # pritn one item\n",
    "#     print(input_ids)\n",
    "#     print(senses_ids)\n",
    "#     print(labels)\n",
    "#     print(attention_mask)\n",
    "\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GlossBERT()\n",
    "# model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# TRAINER\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# wandb_logger = WandbLogger(project='nlp_hw2', log_model=True)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, callbacks\u001b[39m=\u001b[39m[ModelCheckpoint(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m)])\u001b[39m#, logger=wandb_logger)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_loader, val_loader)\n\u001b[1;32m      9\u001b[0m \u001b[39m# trainer = Trainer(max_epochs=10, callbacks=[ModelCheckpoint(monitor='val_loss')])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# trainer.fit(model, train_loader, val_loader)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    730\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    731\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    734\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> 735\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    736\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    737\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:682\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    683\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    769\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    772\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    773\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1139\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restore_modules_and_callbacks(ckpt_path)\n\u001b[1;32m   1138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_configure_sharded_model()  \u001b[39m# allow user to setup in model sharded environment\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49msetup(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1141\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[39m# INSPECT THE CORE LOOPS\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[39mfr\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m \u001b[39m     Lightning internal flow looks like this:\u001b[39m\n\u001b[1;32m   1146\u001b[0m \u001b[39m\u001b[39m\u001b[39m{\u001b[39;00mTrainer\u001b[39m.\u001b[39mfit\u001b[39m}\u001b[39;00m\u001b[39m or \u001b[39m\u001b[39m{\u001b[39;00mTrainer\u001b[39m.\u001b[39mtest\u001b[39m}\u001b[39;00m\u001b[39m or \u001b[39m\u001b[39m{\u001b[39;00mTrainer\u001b[39m.\u001b[39mpredict\u001b[39m}\u001b[39;00m\u001b[39m  ||\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[39m`pytorch_lightning/plugins/training_type_plugin` to find accelerator dispatch functions.\u001b[39m\n\u001b[1;32m   1167\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/accelerators/gpu.py:46\u001b[0m, in \u001b[0;36mGPUAccelerator.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_nvidia_flags(trainer\u001b[39m.\u001b[39mlocal_rank)\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msetup(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:91\u001b[0m, in \u001b[0;36mAccelerator.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Setup plugins for the trainer fit and creates optimizers.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m        trainer: the trainer instance\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_training_type_plugin()\n\u001b[1;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39msetup_optimizers_in_pre_dispatch:\n\u001b[1;32m     93\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup_optimizers(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:360\u001b[0m, in \u001b[0;36mAccelerator.setup_training_type_plugin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup_training_type_plugin\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Attaches the training type plugin to the accelerator.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49msetup()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/single_device.py:71\u001b[0m, in \u001b[0;36mSingleDevicePlugin.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_to_device()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/single_device.py:68\u001b[0m, in \u001b[0;36mSingleDevicePlugin.model_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel_to_device\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py:109\u001b[0m, in \u001b[0;36mDeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_parse_to(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__update_properties(device\u001b[39m=\u001b[39mout[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mout[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 109\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:852\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    850\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 852\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:530\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    529\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 530\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    532\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    533\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    534\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    535\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:530\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    529\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 530\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    532\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    533\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    534\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    535\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:530\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    529\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 530\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    532\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    533\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    534\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    535\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:552\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m param \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 552\u001b[0m         param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    553\u001b[0m     should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    554\u001b[0m     \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2023-hw2/lib/python3.9/site-packages/torch/nn/modules/module.py:850\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    848\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 850\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# TRAINER\n",
    "# wandb_logger = WandbLogger(project='nlp_hw2', log_model=True)\n",
    "trainer = Trainer(gpus=0, max_epochs=10, callbacks=[ModelCheckpoint(monitor='val_loss')])#, logger=wandb_logger)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "# trainer = Trainer(max_epochs=10, callbacks=[ModelCheckpoint(monitor='val_loss')])\n",
    "# trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
